# Gu√≠a de Full Sync - Soluci√≥n de Duplicados en BigQuery

## üîç Problema Identificado

### S√≠ntoma
Los datos en BigQuery se estaban duplicando. Por ejemplo, una empleada que registr√≥ 40 horas en Clockify aparec√≠a con 102 horas en PowerBI.

### Causa Ra√≠z
El sistema usaba la funci√≥n `hash()` de Python para generar IDs num√©ricos a partir de los IDs de Clockify. El problema es que **`hash()` no es determin√≠stico** en Python 3.3+:

- Al reiniciar el servicio, la misma entrada de tiempo generaba un ID diferente
- El MERGE en BigQuery no reconoc√≠a el registro como existente
- Se insertaba como nuevo en lugar de actualizar ‚Üí **duplicados masivos**

## ‚úÖ Soluci√≥n Implementada

### 1. Hash Determin√≠stico (MD5)
**Archivo modificado:** `clockify_transformer.py`

Se reemplaz√≥ `hash()` por `hashlib.md5()` que es determin√≠stico:

```python
# ANTES (problem√°tico):
numeric_id = abs(hash(clockify_id)) % (10**10)

# DESPU√âS (correcto):
numeric_id = _generate_deterministic_id(clockify_id)
```

**Resultado:** El mismo time entry de Clockify siempre genera el mismo ID, sin importar cu√°ntas veces se reinicie el servicio.

### 2. Mecanismo de Full Sync
**Archivos modificados:** `main.py`, `bq_utils.py`

Se a√±adi√≥ soporte para truncar (borrar) las tablas antes de recargar, eliminando todos los duplicados existentes.

## üöÄ C√≥mo Ejecutar el Full Sync

### Opci√≥n 1: Variable de Entorno (Recomendado)

```bash
# Activar FULL_SYNC para limpiar duplicados
FULL_SYNC=true python main.py
```

O si usas Docker/Cloud Run:

```bash
# En .env o configuraci√≥n del servicio
FULL_SYNC=true
```

### Opci√≥n 2: Configuraci√≥n Temporal

```bash
# Ejecutar una sola vez con full sync
export FULL_SYNC=true
python main.py

# Desactivar para siguientes ejecuciones
unset FULL_SYNC
```

### Opci√≥n 3: Cloud Run / Kubernetes

Actualizar la variable de entorno en la configuraci√≥n del servicio:

```yaml
env:
  - name: FULL_SYNC
    value: "true"
```

## ‚ö†Ô∏è Advertencias Importantes

1. **El Full Sync borra TODAS las tablas configuradas** en `endpoints.yaml` antes de recargar
2. **Solo ejecutar cuando sea necesario** (despu√©s de desplegar el fix del hash, o para limpiar duplicados)
3. **Tarda m√°s tiempo** que un sync normal porque recarga todo el hist√≥rico
4. **Despu√©s del full sync, desactivar FULL_SYNC** para volver a syncs incrementales

## üìä Proceso del Full Sync

```
1. Detectar FULL_SYNC=true
   ‚Üì
2. Para cada tabla en endpoints.yaml:
   ‚îú‚îÄ‚îÄ Truncar tabla target (borrar todo)
   ‚îú‚îÄ‚îÄ Obtener datos de Clockify/Runn
   ‚îú‚îÄ‚îÄ Cargar a tabla staging
   ‚îî‚îÄ‚îÄ MERGE (INSERT todo, ya que target est√° vac√≠o)
   ‚Üì
3. Resultado: Datos limpios sin duplicados
```

## üîß Flujo Recomendado para Limpiar Duplicados

### Paso 1: Hacer Full Sync (UNA VEZ)
```bash
# Ejecutar con full sync para limpiar duplicados
FULL_SYNC=true python main.py
```

Ver√°s este mensaje:
```
============================================================
‚ö†Ô∏è  FULL SYNC ACTIVADO - Se borrar√°n todas las tablas antes de recargar
============================================================

[runn_actuals] FULL SYNC activado - truncando tabla...
[runn_actuals] full sync: 2000 filas desde Clockify

‚úÖ Full sync completado - Todos los duplicados han sido eliminados
```

### Paso 2: Desactivar Full Sync
```bash
# Volver a modo normal (solo para siguientes ejecuciones)
unset FULL_SYNC
```

O remover la variable de tu configuraci√≥n en Cloud Run.

### Paso 3: Verificar en PowerBI
- Las horas ahora deben coincidir con Clockify
- No debe haber duplicados

## üîÑ Syncs Futuros (Modo Normal)

Despu√©s del full sync inicial, los syncs normales funcionar√°n correctamente gracias al hash determin√≠stico:

```bash
# Sin FULL_SYNC (modo normal)
python main.py

# Output esperado:
[runn_actuals] upsert: 2000 filas desde Clockify
```

El MERGE ahora funciona correctamente:
- Actualiza registros existentes
- Inserta solo los nuevos
- **Sin duplicados**

## üìù Notas T√©cnicas

### ¬øPor qu√© MD5 en lugar de otro hash?

- **Determin√≠stico**: Siempre genera el mismo hash para el mismo input
- **R√°pido**: Suficiente para generar IDs
- **Ampliamente soportado**: Disponible en todas las versiones de Python
- **No es para seguridad**: Solo para generar IDs √∫nicos consistentes

### Tabla de Cambios

| Archivo | Cambio | Prop√≥sito |
|---------|--------|-----------|
| `clockify_transformer.py` | `hash()` ‚Üí `hashlib.md5()` | IDs determin√≠sticos |
| `bq_utils.py` | A√±adir `truncate_table()` | Limpiar tablas |
| `main.py` | A√±adir soporte `FULL_SYNC` | Control de full sync |

## üÜò Troubleshooting

### Los duplicados siguen apareciendo
- Verificar que el c√≥digo actualizado est√© desplegado
- Ejecutar full sync con `FULL_SYNC=true`
- Verificar que no haya m√∫ltiples procesos escribiendo a BigQuery

### El full sync falla
- Verificar permisos en BigQuery (necesita TRUNCATE TABLE)
- Revisar logs para errores espec√≠ficos
- Verificar que las tablas existan

### ¬øCu√°ndo usar full sync?
- **Despu√©s de desplegar el fix del hash** (primera vez)
- **Si detectas duplicados** en los datos
- **Si cambias la l√≥gica de IDs** en el transformer
- **NO usar en syncs regulares** (m√°s lento e innecesario)

## üìû Contacto

Si encuentras problemas, revisa los logs y verifica:
1. C√≥digo actualizado desplegado
2. FULL_SYNC activado correctamente
3. Permisos en BigQuery
4. Sin otros procesos escribiendo simult√°neamente
