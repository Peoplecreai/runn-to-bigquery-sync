# Soluci√≥n al Problema de Duplicaci√≥n 2.6x en PowerBI

## üìä Resumen del Problema

**S√≠ntomas:**
- Dashboard de PowerBI reporta 2.6x m√°s horas de las reales
- Ejemplo: Marcela Aburto semana 10-14 Nov 2025
  - Real (Clockify): 40 horas (23 entries)
  - PowerBI reporta: 103 horas
  - Factor: 2.575x

## üîç Causa Ra√≠z Identificada

El problema tiene **DOS niveles de duplicaci√≥n**:

### 1. Duplicados en la API de Clockify
La API de Clockify devuelve el MISMO time entry m√∫ltiples veces cuando se itera por usuarios:
- Un time entry puede aparecer 2-3 veces si est√° asociado con m√∫ltiples usuarios
- Esto causaba que el mismo registro se procesara m√∫ltiples veces

**Status:** ‚úÖ **YA SOLUCIONADO** en `clockify_client.py:98-100`

### 2. Duplicados Hist√≥ricos en BigQuery
Los duplicados de ejecuciones ANTERIORES (antes de implementar la deduplicaci√≥n en nivel 1) se acumularon en BigQuery:
- El MERGE usaba `id` (hash num√©rico) como clave √∫nica
- Si hab√≠a m√∫ltiples rows con el mismo `_clockify_id` pero diferentes `id`, no se detectaban como duplicados
- Los MERGE sucesivos NO eliminaban estos duplicados hist√≥ricos

**Status:** üî¥ **REQUIERE LIMPIEZA** - este es el problema actual

## ‚úÖ Soluci√≥n Implementada

### Cambios en el C√≥digo

#### 1. **bq_utils.py** - MERGE mejorado
- **Antes:** MERGE por `id` (hash num√©rico)
- **Ahora:** MERGE por `_clockify_id` (ID real de Clockify)
- Agregado: Deduplicaci√≥n autom√°tica del staging antes del merge
- Agregado: Funci√≥n `deduplicate_table_by_column()` para limpiar duplicados hist√≥ricos

**Archivo:** `bq_utils.py:47-75`

#### 2. **main.py** - Sincronizaci√≥n con limpieza autom√°tica
- **Antes:** Solo hac√≠a MERGE sin verificar duplicados
- **Ahora:**
  - Limpia duplicados hist√≥ricos ANTES del merge
  - Usa `_clockify_id` como clave √∫nica para Clockify
  - Deduplica autom√°ticamente en cada sincronizaci√≥n

**Archivo:** `main.py:103-114`

#### 3. **fix_duplicates_now.py** - Script de limpieza inmediata
- Script standalone para limpiar duplicados AHORA MISMO
- No requiere esperar a la pr√≥xima sincronizaci√≥n
- Reporta estad√≠sticas antes/despu√©s

### Protecciones Implementadas

El pipeline ahora tiene **3 capas de protecci√≥n** contra duplicados:

1. **Capa 1 - API de Clockify** (`clockify_client.py:68-121`)
   - Deduplica por Clockify ID al obtener datos de la API
   - Reporta estad√≠sticas de duplicados detectados

2. **Capa 2 - Transformaci√≥n** (`main.py:53-89`)
   - Verifica IDs duplicados despu√©s de transformar
   - Detecta colisiones de hash MD5

3. **Capa 3 - BigQuery** (`main.py:103-114`, `bq_utils.py:61-69`)
   - Limpia duplicados hist√≥ricos antes del merge
   - Deduplica staging antes del merge
   - Usa `_clockify_id` como clave √∫nica

## üöÄ C√≥mo Aplicar el Fix

### Opci√≥n 1: Limpieza Inmediata (RECOMENDADO)

Ejecuta el script de limpieza inmediata:

```bash
# Configurar credenciales de BigQuery
export BQ_PROJECT="tu-proyecto-gcp"
export BQ_DATASET="people_analytics"

# Ejecutar limpieza
python fix_duplicates_now.py
```

**Resultado esperado:**
```
‚ö†Ô∏è  Duplicados detectados en project.people_analytics.runn_actuals:
   Total rows: 60 (ejemplo)
   Rows √∫nicos: 23
   Duplicados a eliminar: 37
   Factor de duplicaci√≥n: 2.61x

‚úÖ Deduplicaci√≥n completada: 60 ‚Üí 23 rows
```

### Opci√≥n 2: Full Sync

Alternativamente, puedes hacer un full sync que borra todo y recarga desde cero:

```bash
export FULL_SYNC=true
python main.py
```

‚ö†Ô∏è **Advertencia:** Esto borrar√° TODA la tabla y recargar√° todos los datos de Clockify (√∫ltimos 90 d√≠as)

### Opci√≥n 3: Sincronizaci√≥n Normal

La pr√≥xima sincronizaci√≥n normal ya incluye limpieza autom√°tica:

```bash
python main.py
```

El script autom√°ticamente:
1. Detectar√° duplicados existentes
2. Los limpiar√° antes del merge
3. Usar√° `_clockify_id` para evitar duplicados futuros

## üß™ Validaci√≥n

### 1. Verificar limpieza en BigQuery

```sql
-- Contar registros para Marcela Aburto (semana 10-14 Nov 2025)
SELECT
    COUNT(*) as total_registros,
    COUNT(DISTINCT _clockify_id) as clockify_ids_unicos,
    SUM(billableMinutes) / 60.0 as billable_hours,
    SUM(nonbillableMinutes) / 60.0 as nonbillable_hours,
    (SUM(billableMinutes) + SUM(nonbillableMinutes)) / 60.0 as total_hours
FROM `project.people_analytics.runn_actuals`
WHERE date BETWEEN '2025-11-10' AND '2025-11-14'
  AND personId IN (
    SELECT id FROM `project.people_analytics.runn_people`
    WHERE LOWER(firstName || ' ' || lastName) LIKE '%marcela%aburto%'
  )
```

**Esperado:**
- `total_registros` = `clockify_ids_unicos` = 23
- `total_hours` ‚âà 40.0 horas

### 2. Verificar en PowerBI

1. Refresca tu dataset de PowerBI
2. Verifica las horas de Marcela Aburto (10-14 Nov 2025)
3. Deber√≠a mostrar ~40 horas (no 103)

### 3. Script de debugging

Usa el script de debugging existente para an√°lisis detallado:

```bash
python debug_duplicates.py
```

Este script verifica:
- Conteo de registros vs IDs √∫nicos
- Suma de horas en BigQuery
- Duplicados por ID
- Colisiones de hash
- Distribuci√≥n por fecha

## üìã Prevenci√≥n Futura

Los cambios implementados previenen duplicados futuros autom√°ticamente:

‚úÖ **Deduplicaci√≥n en API** - Ya no se obtienen duplicados de Clockify
‚úÖ **MERGE por Clockify ID** - Usa el ID real como clave √∫nica
‚úÖ **Limpieza autom√°tica** - Elimina duplicados hist√≥ricos en cada sync
‚úÖ **Deduplicaci√≥n de staging** - Verifica staging antes del merge

**No se requiere acci√≥n adicional** - el pipeline ya est√° corregido.

## üîß Archivos Modificados

1. `bq_utils.py`
   - Nueva funci√≥n: `deduplicate_table_by_column()`
   - MERGE mejorado con deduplicaci√≥n autom√°tica
   - Soporte para claves √∫nicas custom (no solo `id`)

2. `main.py`
   - Limpieza de duplicados antes del merge para Clockify
   - Uso de `_clockify_id` como clave √∫nica
   - Import de `deduplicate_table_by_column`

3. `fix_duplicates_now.py` (NUEVO)
   - Script de limpieza inmediata
   - Reporta estad√≠sticas antes/despu√©s

4. `SOLUTION_DUPLICADOS.md` (NUEVO)
   - Este documento de soluci√≥n completa

## ‚ùì FAQ

### ¬øPor qu√© 2.6x espec√≠ficamente?

Si la API de Clockify devuelve cada time entry ~2.6 veces en promedio (algunos 2x, otros 3x), y estos duplicados se acumularon en m√∫ltiples sincronizaciones, el factor de duplicaci√≥n final es ~2.6x.

### ¬øSe perder√°n datos al limpiar duplicados?

No. La deduplicaci√≥n mantiene el registro M√ÅS RECIENTE (por `updatedAt`) de cada time entry √∫nico. Solo elimina copias exactas del mismo time entry.

### ¬øAfectar√° esto a mis reportes hist√≥ricos?

S√≠, en el sentido de que **corregir√°** los n√∫meros. Los reportes hist√≥ricos que antes mostraban 2.6x m√°s horas ahora mostrar√°n las horas correctas.

### ¬øNecesito ejecutar el fix cada vez?

No. Solo necesitas ejecutar `fix_duplicates_now.py` UNA VEZ para limpiar duplicados hist√≥ricos. Despu√©s, el pipeline normal (`main.py`) ya previene duplicados futuros autom√°ticamente.

### ¬øQu√© pasa si vuelvo a ejecutar una sincronizaci√≥n antigua?

El MERGE ahora usa `_clockify_id` como clave √∫nica, por lo que aunque ejecutes el mismo time entry m√∫ltiples veces, solo se actualizar√° (no se duplicar√°).

## üìû Soporte

Si despu√©s de aplicar el fix sigues viendo duplicados:

1. Ejecuta `python debug_duplicates.py` para an√°lisis detallado
2. Verifica que las variables de entorno est√©n configuradas correctamente
3. Revisa los logs de la sincronizaci√≥n para errores
4. Contacta al equipo de desarrollo con los logs

---

**Fecha de soluci√≥n:** 2025-11-19
**Autor:** Claude Code
**Estado:** ‚úÖ Listo para implementaci√≥n
